\documentclass[12pt]{article}
\usepackage{a4wide, amsfonts, epsfig,bbding}

\begin{document}
\begin{center}
{\bf EMAT10001 Lecture 14.}\\[1cm]{} Conor Houghton 2014-2-1
\end{center}
\subsection*{Preface} 
These are outline notes for lecture 14. As usual there is a bounty of
between 20p and \pounds 2 for errors, you can tell me at the end of a
lecture or email me at \texttt{conor.houghton{@}bristol.ac.uk}.

\subsection*{Introduction}

This lecture is about the Taylor series and about the numerical
solution of differential equations; two methods are considered, the
Euler method and the Runge Kutta method.

\subsection*{The Taylor series}

In the last lecture we used a series to describe a function, in that
case the exponential function. In fact, it is often useful to use a
series description of functions, it can help with numerical
calculations of the values and it can be useful in studying properties
of the function. Here, it will be used to work out how to efficiently
calculate the solutions to differential equations accurately. The
Taylor series is one commonly applicable approach to representing a
function as a series.

Imagine you have a function $f(t)$ that can be represented as a series
\begin{equation}
f(t)=\sum_{n=0}^\infty{a_nt^n}=a_0+a_1t+a_2t^2+\ldots
\end{equation}
Now, putting $t=0$ we get
\begin{equation}
f(0)=a_0
\end{equation}
Next, differentiate
\begin{equation}
\frac{df(t)}{dt}=a_1+2a_2t+3a_3t^2+\ldots=\sum_{n=1}^\infty{a_nnt^{n-1}}
\end{equation}
so, putting $t=0$ we get
\begin{equation}
\frac{df}{dt}|_{t=0}=a_1
\end{equation}
Differentiating again gives
\begin{equation}
\frac{d^2f(t)}{dt^2}=2a_2+6a_3t+12a_4t^2\ldots=\sum_{n=1}^\infty{a_nn(n-1)t^{n-2}}
\end{equation}
so
\begin{equation}
\frac{1}{2}\frac{d^2f}{dt^2}|_{t=0}=a_2
\end{equation}
and so on.

In fact, by this sort of calculation we see that
\begin{equation}
a_n=\frac{1}{n!}\left.\frac{d^nf}{dt^n}\right|_{t=0}
\end{equation}
or, put another way, 
\begin{equation}
f(t)=\sum_{n=0}^\infty\frac{1}{n!}\left.\frac{d^nf}{dt^n}\right|_{t=0}t^n
\end{equation}
This is the Taylor series. We haven't proven that $f(t)$ has a series
of the form $\sum_{n=0}^\infty a_nt^n$ and not all functions do, in
particular, if the function is badly behaved at $t=0$ it may not. We
also haven't proved that the series converges. If we write
\begin{equation}
f(t)=\sum_{n=0}^{N-1}\frac{1}{n!}\left.\frac{d^nf}{dt^n}\right|_{t=0}t^n+E_N(t)
\end{equation}
where $E_N(t)$ represents the error from stopping at after $N$ terms,
we might expect $E_N(t)$ vanishes as $N$ goes to infinity. In fact,
this doesn't always happen and sometimes, even when the series does
converge, it does so very slowly, so $E_N(t)$ remains large even for
very large values of $N$. Frequently the series converges for some
values of $t$ but not for others, we will see an example of this in
the worksheet. Nonetheless, the Taylor series is frequently useful.

\subsection*{The Taylor series - examples}

We already know the series expansion of $\exp{t}$, but lets calculate
it as a Taylor series. Since 
\begin{equation}
\frac{d}{dt}\exp{t}=\exp{t}
\end{equation}
we know
\begin{equation}
\frac{d^n}{dt^n}\exp{t}=\exp{t}
\end{equation}
or
\begin{equation}
\left.\frac{d^n}{dt^n}\exp{t}\right|_{t=0}=1
\end{equation}
for all $n$ so
\begin{equation}
f(t)=\sum_{n=0}^{\infty}\frac{1}{n!}t^n
\end{equation}
which is what we got before.

Next lets consider
\begin{equation}
f(t)=\sin{t}
\end{equation}
Now 
\begin{equation}
\frac{d}{dt}f(t)=\cos{t}
\end{equation}
and
\begin{equation}
\frac{d^2}{dt^2}f(t)=-\sin{t}
\end{equation}
and so on. Putting $t=0$ and using $\sin{0}=0$ and $\cos{0}=1$ we get
\begin{equation}
\sin{t}=\sum_{n\,\mbox{odd}}\frac{(-1)^{(n-1)/2}t^n}{n!}
\end{equation}

Finally, we have been expanding around $t=0$, but you can expand around any point, here we expand around $t=t_0$
\begin{equation}
f(t)=\sum_{n=0}^\infty\frac{1}{n!}\left.\frac{d^nf}{dt^n}\right|_{t=t_0}(t-t_0)^n
\end{equation}
or, writing $\epsilon=t-t_0$
\begin{equation}
f(t_0+\epsilon)=\sum_{n=0}^\infty\frac{1}{n!}\left.\frac{d^nf}{dt^n}\right|_{t=t_0}\epsilon^n
\end{equation}

\subsection*{Numerical solutions of differential equations}

Consider the differential equation
\begin{equation}
\frac{dy}{dt}=f(y)
\end{equation}
This class of differential equations would include the growth equation we looked at last week:
\begin{equation}
\frac{dy}{dt}=ry
\end{equation}
with $f(y)=ry$. Of course, the right hand side might also depend on
$t$, but we'll worry about that later. Imagine we want to find
numerical values for $y(t)$ where we know $y(0)=y_0$, some value, and
\begin{equation}
\frac{dy}{dt}=f(y)
\end{equation}
Imagine further that we can't solve the equation analytically as we can
for the growth equation, so we resort to solving it approximately on
the computer. The normal approach would be to discretize time and to
work out the solution approximately for each time step in turn,
depending on the previous time step. In other words, say we choose
$\delta t$, a small value, as the time step then we would work out
$y(\delta t)$, then use that to work out $y(2\delta t)$ and so on.

Let us use the notation $y_n=y(n\delta t)$ and consider how we might
get a computer to work out $y_{n+1}$ approximately if $y_n$ is already
known. Now, by the Taylor expansion
\begin{equation}
y(n\delta t+\delta t)=y(n\delta
t)+\left.\frac{dy}{dt}\right|_{t=n\delta t}\delta
t+\frac{1}{2}\left.\frac{d^2y}{dt^2}\right|_{t=n\delta t}(\delta t)^2+\ldots
\end{equation}
so one simple approach is to ignore the $(\delta t)^2$ and smaller terms, since $dy/dt=f(y)$ this gives
\begin{equation}
y_{n+1}=y_n+f(y_n)\delta t
\end{equation}
This approximation is known as the \textsl{Euler method}. A simple example is plotted in Fig.~1.

\begin{figure}
\begin{center}
\include{euler_approx}
\end{center}
\caption{A comparison of the Euler method with the true solution for
  the growth equation. This equation in actually one where the Euler
  method works quite well for modest values of $t$, here a large time
  step of $\delta t=0.2$ is used to emphasise the error, the growth
  rate is $r=0.5$ and the actual solution is plotted for comparison.}
\end{figure}

\subsection*{The Runge-Kutta method}

The Runge-Kutta method uses the Taylor expansion in a clever way to
find a better approximation than the euler expansion. It is a bit
convoluted, so there is a lot of notation, but it does give a very
useful numerical algorithm.

As before, we want to solve
\begin{equation}
\frac{dy}{dt}=f(y)
\end{equation}
with a time discretization of $\delta t$, $y_n$ is the approximate
value the algorithm calculates for $y(n\delta t)$ and $y_0=y(0)$, the
initial condition. Now say we are at $y_n$ and let
\begin{equation}
k_1=f(y_n)
\end{equation}
so the Euler approximation would be $y_{n+1}=y_n+k_1\delta t$. Next, let
\begin{equation}
k_2=f\left(y_n+\frac{\delta t k_1}{2}\right)
\end{equation}
Now, using the Taylor expansion
\begin{equation}
k_2=f(y_n+\delta t k_1/2)=f(y_n)+\left.\frac{df}{dy}\right|_{y=y_n}\delta t\frac{k_1}{2}+\ldots
\end{equation}
Substituting back for $k_1$ this gives
\begin{equation}
k_2=f(y_b)+\frac{1}{2}\left.\frac{df}{dy}\right|_{y=y_n}\left.\frac{dy}{dt}\right|_{t=t_n}\delta t+\ldots
\end{equation}
Using the chain rule
\begin{equation}
\frac{d^2y}{dt^2}=\frac{df}{dt}=\frac{df}{dy}\frac{dy}{dt}
\end{equation}
so
\begin{equation}
k_2=f(y_b)+\frac{1}{2}\frac{d^2y}{dt^2}\delta t+\ldots
\end{equation}
Now, recall
\begin{equation}
y(n\delta t + \delta t)=y_n+\left.\frac{dy}{dt}\right|_{t=t_n}\delta t+\frac{1}{2}\left.\frac{d^2y}{dt^2}\right|_{t=t_n}\delta t^2+\ldots
\end{equation}
and from the formula for $k_1$ and $k_2$ we see that this can be written as
\begin{equation}
y_{n+1}=y_n+k_2\delta t
\end{equation}
This is the \textsl{second order Runge Kutta method}, it is called second order
because it include the first and second order terms in the Taylor
expansion, the Euler method is like a first order Runge Kutta method. 

The second order Runge Kutta method isn't usually used; it is the
fourth order Runge Kutta that is considered the standard way of doing
numerical integration. The idea is just the same as the one we saw
above, by combining different terms more of the Taylor expansion is
accounted for, in fact, as the name suggests, the fourth order Runge
Kutta gets everything up to the fourth order, the errors are like
$\delta t^5$.

Here I will give the fourth order Runge Kutta and will include the
possibility that the right hand side of the differential equation also
includes a dependence on $t$ so, writing $t_n=n\delta t$
\begin{equation}
\frac{dy}{dt}=f(t,y)
\end{equation}
Now
\begin{eqnarray}
k_1&=&f(t_n,y_n)\cr
k_2&=&f\left(t_n+\frac{1}{2}\delta,y_n+\frac{1}{2}\delta tk_1\right)\cr 
k_3&=&f\left(t_n+\frac{1}{2}\delta,y_n+\frac{1}{2}\delta tk_2\right)\cr 
k_4&=&f\left(t_n+\delta,y_n+\delta tk_2\right) 
\end{eqnarray}
and 
\begin{equation}
y_{n+1}=y_n+\frac{1}{6}(k_1+2k_2+2k_3+k_4)
\end{equation}

\begin{figure}
\begin{center}
\include{rk_approx}
\end{center}
\caption{A comparison of the fourth order Runge Kutta method with the
  true solution for the growth equation. This has the same values of
  $r$ and $\delta t$ as in Fig.~1, but the fourth order Runge-Kutta
  method is used instead of the Euler method. As you can see, the
  approximation and true solution are indistinguishable.}
\end{figure}


\end{document}

